# Enumerate content & functionality
- [Web spidering](#web-spidering-user-directed-spidering)
- [Discovering hidden content](#discovering-hidden-content)
- [Inference from published content](#inference-from-published-content)
- [Use of public informations](#use-of-public-informations)

## Web spidering (User-directed spidering)
```
1. Configure your browser to use either Burp or WebScarab as a local proxy

2. Browse the entire application normally, attempting to visit every link/URL
you discover, submitting every form, and proceeding through all multi-
step functions to completion. Try browsing with JavaScript enabled and
disabled, and with cookies enabled and disabled. Many applications can
handle various browser configurations, and you may reach different con-
tent and code paths within the application.

3. Review the site map generated by the proxy/spider tool, and identify
any application content or functions that you did not browse manually.
Establish how the spider enumerated each item. For example, in Burp
Spider, check the Linked From details. Using your browser, access the item
manually so that the response from the server is parsed by the proxy/spi-
der tool to identify any further content. Continue this step recursively until
no further content or functionality is identified.

4. Optionally, tell the tool to actively spider the site using all of the already
enumerated content as a starting point. To do this, first identify any URLs
that are dangerous or likely to break the application session, and config-
ure the spider to exclude these from its scope. Run the spider and review
the results for any additional content it discovers.
The site map generated by the proxy/spider tool contains a wealth of infor-
mation about the target application, which will be useful later in identifying
the various attack surfaces exposed by the application
```

## Discovering hidden content
```
1. Make some manual requests for known valid and invalid resources, and
identify how the server handles the latter.

2. Use the site map generated through user-directed spidering as a basis for
automated discovery of hidden content.

3. Make automated requests for common filenames and directories within
each directory or path known to exist within the application. Use Burp
Intruder or a custom script, together with wordlists of common files and
directories, to quickly generate large numbers of requests. If you have iden-
tified a particular way in which the application handles requests for invalid
resources (such as a customized “file not found” page), configure Intruder
or your script to highlight these results so that they can be ignored.

4. Capture the responses received from the server, and manually review
them to identify valid resources.

5. Perform the exercise recursively as new content is discovered
```

## Inference from published content
```
1. Review the results of your user-directed browsing and basic brute-force
exercises. Compile lists of the names of all enumerated subdirectories, file
stems, and file extensions.

2. Review these lists to identify any naming schemes in use. For example, if
there are pages called AddDocument.jsp and ViewDocument.jsp, there
may also be pages called EditDocument.jsp and RemoveDocument.jsp.
You can often get a feel for developers’ naming habits just by reading a
few examples. For example, depending on their personal style, develop-
ers may be verbose (AddANewUser.asp), succinct (AddUser.asp), use
abbreviations (AddUsr.asp), or even be more cryptic (AddU.asp). Getting
a feel for the naming styles in use may help you guess the precise names
of content you have not already identified.

3. Sometimes, the naming scheme used for different content employs
identifiers such as numbers and dates, which can make inferring hidden
content easy. This is most commonly encountered in the names of static
resources, rather than dynamic scripts. For example, if a company’s web-
site links to AnnualReport2009.pdf and AnnualReport2010.pdf,
it should be a short step to identifying what the next report will be called.
Somewhat incredibly, there have been notorious cases of companies
placing files containing financial reports on their web servers before they
were publicly announced, only to have wily journalists discover them
based on the naming scheme used in earlier years.

4. Review all client-side code such as HTML and JavaScript to identify any
clues about hidden server-side content. These may include HTML com-
ments related to protected or unlinked functions, HTML forms with dis-
abled SUBMIT elements, and the like. Often, comments are automatically
generated by the software that has been used to generate web content,
or by the platform on which the application is running. References to
items such as server-side include files are of particular interest. These
files may actually be publicly downloadable and may contain highly sensi-
tive information such as database connection strings and passwords. In
other cases, developers’ comments may contain all kinds of useful tidbits,
such as database names, references to back-end components, SQL query
strings, and so on. Thick-client components such as Java applets and
ActiveX controls may also contain sensitive data that you can extract.

5. Add to the lists of enumerated items any further potential names con-
jectured on the basis of the items that you have discovered. Also add to
the file extension list common extensions such as txt, bak, src, inc,
and old, which may uncover the source to backup versions of live pages.
Also add extensions associated with the development languages in use,
such as .java and .cs, which may uncover source files that have been
compiled into live pages. (See the tips later in this chapter for identifying
technologies in use.)

6. Search for temporary files that may have been created inadvertently by
developer tools and file editors. Examples include the .DS_Store file,
which contains a directory index under OS X, file.php~1, which is a
temporary file created when file.php is edited, and the .tmp file exten-
sion that is used by numerous software tools.

7. Perform further automated exercises, combining the lists of directories,
file stems, and file extensions to request large numbers of potential
resources. For example, in a given directory, request each file stem com-
bined with each file extension. Or request each directory name as a subdi-
rectory of every known directory.

8. Where a consistent naming scheme has been identified, consider perform-
ing a more focused brute-force exercise. For example, if AddDocument
.jsp and ViewDocument.jsp are known to exist, you may create
a list of actions (edit, delete, create) and make requests of the form
XxxDocument.jsp. Alternatively, create a list of item types (user, account,
file) and make requests of the form AddXxx.jsp.

9. Perform each exercise recursively, using new enumerated content and
patterns as the basis for further user-directed spidering and further auto-
mated content discovery. You are limited only by your imagination, time
available, and the importance you attach to discovering hidden content
within the application you are targeting.
```

## Use of public informations
```
1. Use several different search engines and web archives (listed previously)
to discover what content they indexed or stored for the application you
are attacking.

2. When querying a search engine, you can use various advanced techniques
to maximize the effectiveness of your research. The following suggestions
apply to Google. You can find the corresponding queries on other engines
by selecting their Advanced Search option.
  - site:www.wahh-target.com returns every resource within the target site
    that Google has a reference to.
  - site:www.wahh-target.com login returns all the pages containing the expression
    login. In a large and complex application, this technique can
    be used to quickly home in on interesting resources, such as site maps, password
    reset functions, and administrative menus.
  - link:www.wahh-target.com returns all the pages on other websites and applications
    that contain a link to the target. This may include links
    to old content, or functionality that is intended for use only by third parties, such
    as partner links.
  - related:www.wahh-target.com returns pages that are “similar” to the target and therefore
    includes a lot of irrelevant material. However, it may
    also discuss the target on other sites, which may be of interest.

3. Perform each search not only in the default Web section of Google, but
also in Groups and News, which may contain different results.

4. Browse to the last page of search results for a given query, and select
Repeat the Search with the Omitted Results Included. By default, Google
attempts to filter out redundant results by removing pages that it believes
are sufficiently similar to others included in the results. Overriding this
behavior may uncover subtly different pages that are of interest to you
when attacking the application.

5. View the cached version of interesting pages, including any content that is
no longer present in the actual application. In some cases, search engine
caches contain resources that cannot be directly accessed in the applica-
tion without authentication or payment

6. Perform the same queries on other domain names belonging to the same
organization, which may contain useful information about the application
you are targeting.

7. Compile a list containing every name and e-mail address you can discover
relating to the target application and its development. This should include
any known developers, names found within HTML source code, names found
in the contact information section of the main company website, and any
names disclosed within the application itself, such as administrative staff.

8. Using the search techniques described previously, search for each identi-
fied name to find any questions and answers they have posted to Internet
forums. Review any information found for clues about functionality or vul-
nerabilities within the target application
```
### Extra note
If your research identifies old content and functionality that is no longer
linked to within the main application, it may still be present and usable. The
old functionality may contain vulnerabilities that do not exist elsewhere
within the application.
Even where old content has been removed from the live application, the
content obtained from a search engine cache or web archive may contain
references to or clues about other functionality that is still present within the
live application and that can be used to attack it
